import streamlit as st
import pandas as pd
import io
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Set up Streamlit UI
st.set_page_config(layout="wide", page_title="NABLS-AI Dashboard")
st.title('NABLS-AI: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙÙŠ Ø£Ø¨Ø­Ø§Ø« Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ')
st.info('NABLS-AI: Ø£Ø¯Ø§Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙÙŠ Ø£Ø¨Ø­Ø§Ø« Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.')

# --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ø³Ø§Ø± Ø«Ø§Ø¨Øª (Ø¯Ø§Ø®Ù„ Ø§Ù„Ø±ÙŠØ¨Ùˆ) ---
st.header("Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ¯Ø± ğŸ“Š")

# === Ù‡Ø§Ù…: ØªØ£ÙƒØ¯ Ø£Ù† Ù…Ù„Ù 'ai_models.csv' Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù†ÙØ³ Ù…Ø¬Ù„Ø¯ Ù…Ù„Ù ØªØ·Ø¨ÙŠÙ‚ Streamlit Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ===
# Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ ÙØ±Ø¹ÙŠØŒ Ù…Ø«Ù„Ø§Ù‹ 'data/ai_models.csv'
file_path = 'ai_models.csv' # Ø¥Ø°Ø§ ÙƒØ§Ù† ÙÙŠ Ù†ÙØ³ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# file_path = 'data/ai_models.csv' # Ø¥Ø°Ø§ ÙƒØ§Ù† ÙÙŠ Ù…Ø¬Ù„Ø¯ ÙØ±Ø¹ÙŠ Ø§Ø³Ù…Ù‡ 'data'

try:
    df = pd.read_csv(file_path)
    st.success(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù†: `{file_path}`")
    st.subheader("Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©")
    st.dataframe(df.head())
except FileNotFoundError:
    st.error(f"Ø®Ø·Ø£: Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­ ÙˆÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¹Ù„Ù‰ GitHub.")
    st.stop() # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªÙ†ÙÙŠØ° Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù

# --- Ù…Ø­Ø±Ø± Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ¹Ø¯ÙŠÙ„ ---
st.header("ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª âœï¸")
st.markdown("""
    ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ ÙÙŠ Ø§Ù„Ø£Ø³ÙÙ„ Ù„ØªÙ†ÙÙŠØ° ØªØ­Ù„ÙŠÙ„Ø§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ©.
    **Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‡Ø§Ù…Ø©:**
    * **Ù„Ø§ ØªÙ‚Ù… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ `df` Ù…Ù† Ù…Ù„ÙØ§Øª CSV Ø£Ø®Ø±Ù‰ Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙˆØ¯ Ù…Ø§ Ù„Ù… ÙŠÙƒÙ† Ø°Ù„Ùƒ Ø¶Ø±ÙˆØ±ÙŠÙ‹Ø§ Ù„Ù„ØºØ§ÙŠØ©.** Ø§Ù„Ù…ØªØºÙŠØ± `df` ÙŠØ­ØªÙˆÙŠ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©.
    * **Ø§Ø³ØªØ®Ø¯Ù… `st.write()` Ù„Ù„Ù†ØµÙˆØµØŒ `st.dataframe()` Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ùˆ `st.pyplot()` Ù„Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©.**
    * **ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø±Ø§Øª Ù…Ø­Ù„ÙŠØ© Ø«Ø§Ø¨ØªØ© (`df.to_csv`) Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ù„Ø£Ù†Ù‡Ø§ Ù„Ù† ØªØ¹Ù…Ù„ ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„ÙˆÙŠØ¨.**
""")

# Default code for the user to edit
cody = '''
# Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„: By Noora, Sedrah, Basmaleh

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ - (Ø¨ÙˆØ§Ø³Ø·Ø© Noora)
# df Ù‡Ùˆ DataFrame Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ø¨Ø§Ù„ÙØ¹Ù„ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.
# Ù„Ø§ ØªÙ‚Ù… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ù†Ø§ Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ù„Ù Ø¢Ø®Ø±.

if 'Confidence' in df.columns:
    df = df.dropna(subset=["Confidence"])
    df['Confidence'] = df['Confidence'].str.strip().str.lower()
    df = df[df['Confidence'] != 'unknown']

    confidence_count = df["Confidence"].count()
    equ = confidence_count * 0.85
    columns_to_drop = []
    for column_name in df.columns:
        current_column_non_null_count = df[column_name].count()
        if current_column_non_null_count < equ:
            columns_to_drop.append(column_name)

    st.write(f"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù„Ù€ Confidence: {df['Confidence'].unique()}")
    if columns_to_drop:
        df.drop(columns=columns_to_drop, axis=1, inplace=True)
        st.write(f"ØªÙ… Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {columns_to_drop}")
    else:
        st.write("Ù„Ù… ÙŠØªÙ… Ø¥Ø³Ù‚Ø§Ø· Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰.")

    for column in df.columns:
        if df[column].isnull().sum() > 0:
            mode_value = df[column].mode()[0]
            df[column].fillna(mode_value, inplace=True)
else:
    st.warning("Ø¹Ù…ÙˆØ¯ 'Confidence' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. ØªÙ… ØªØ®Ø·ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…ÙˆØ¯.")

# **Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø­Ø°Ù `df.to_csv` Ù‡Ù†Ø§ Ù„Ù…Ù†Ø¹ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„ÙˆÙŠØ¨.**


# ØªØ­Ù„ÙŠÙ„ K-Nearest Neighbors (KNN) - (Ø¨ÙˆØ§Ø³Ø·Ø© Noora)
# df Ù‡Ùˆ DataFrame Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡ Ø¨Ø§Ù„ÙØ¹Ù„.
# Ù„Ø§ ØªÙ‚Ù… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ù†Ø§.

if 'Confidence' in df.columns:
    X = df.drop("Confidence", axis=1)
    y = df["Confidence"]

    for col in X.columns:
        if X[col].dtype == 'object':
            X[col] = X[col].astype(str)

    X = pd.get_dummies(X)

    le = LabelEncoder()
    y = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    knn = KNeighborsClassifier(n_neighbors=2)
    knn.fit(X_train, y_train)

    y_pred = knn.predict(X_test)

    st.subheader("Ù†ØªØ§Ø¦Ø¬ Ù†Ù…ÙˆØ°Ø¬ K-Nearest Neighbors (KNN)")
    st.write(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
    st.write(f"Precision: {precision_score(y_test, y_pred, average='weighted', zero_division=0):.2f}")
    st.write(f"Recall: {recall_score(y_test, y_pred, average='weighted', zero_division=0):.2f}")
    st.write(f"F1 Score: {f1_score(y_test, y_pred, average='weighted', zero_division=0):.2f}")
    st.write("Confusion Matrix:")
    st.code(confusion_matrix(y_test, y_pred))
    st.write("Classification Report:")
    st.text(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))

    error_rate = []
    max_k = min(40, len(X_train) // 2)
    if max_k < 1:
        st.warning("Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‚Ù„ÙŠÙ„Ø© Ø¬Ø¯Ù‹Ø§ Ù„Ø¨Ù†Ø§Ø¡ Ù…Ø®Ø·Ø· Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£ Ù„Ù€ KNN.")
    else:
        for i in range(1, max_k + 1):
            knn = KNeighborsClassifier(n_neighbors=i)
            knn.fit(X_train, y_train)
            pred_i = knn.predict(X_test)
            error_rate.append(np.mean(pred_i != y_test))

        fig_knn_error, ax_knn_error = plt.subplots(figsize=(10,6))
        ax_knn_error.plot(range(1, max_k + 1), error_rate, color='blue', linestyle='dashed', marker='o',
                          markerfacecolor='red', markersize=10)
        ax_knn_error.set_title('Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£ Ù…Ù‚Ø§Ø¨Ù„ Ù‚ÙŠÙ…Ø© K')
        ax_knn_error.set_xlabel('Ù‚ÙŠÙ…Ø© K')
        ax_knn_error.set_ylabel('Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£')
        ax_knn_error.grid(True)
        st.pyplot(fig_knn_error)
        plt.close(fig_knn_error)
else:
    st.warning("Ø¹Ù…ÙˆØ¯ 'Confidence' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ KNN.")


# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ø¹Ø¯Ø§Ø¯Ù‡Ø§ (Ø¨ÙˆØ§Ø³Ø·Ø© Sedrah)
# df Ù‡Ùˆ DataFrame Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡ Ø¨Ø§Ù„ÙØ¹Ù„.
# Ù„Ø§ ØªÙ‚Ù… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ù†Ø§.

def classify_trend(model_name):
    if pd.isna(model_name): return 'Other'
    model_name = str(model_name)
    if 'AFM-on-device' in model_name:
        return 'On-Device AI,Closed Source,Efficiency'
    elif 'AFM-server' in model_name:
        return 'Large-Scale,Closed Source,AI Regulation'
    elif 'AlphaProteo' in model_name:
        return 'AI for Science,Closed Source'
    elif 'Amazon Nova Pro' in model_name:
        return 'Multimodal AI,Closed Source,Large-Scale'
    elif 'Cambrian' in model_name:
        return 'Multimodal AI,Open-Source AI,Domain-Specific'
    elif 'CHAI' in model_name:
        return 'AI for Science,Open-Source AI'
    elif 'Claude' in model_name:
        return 'Reasoning-Mathematics,Closed Source,AI Regulation'
    elif 'Computer-Using Agent ' in model_name:
        return 'Robotics AI,Closed Source,Multimodal AI'
    elif 'DeepL' in model_name:
        return 'Domain-Specific,Closed Source'
    elif 'DeepSeek-R1' in model_name:
        return 'Reasoning-Mathematics,Open-Source AI'
    elif 'DeepSeek-V2.5' in model_name:
        return 'Domain-Specific,Open-Source AI'
    elif 'DeepSeek-V3' in model_name:
        return 'Large-Scale,Open-Source AI'
    elif 'Doubao' in model_name:
        return 'Large-Scale,Closed Source'
    elif 'ERNIE' in model_name:
        return 'Multimodal AI,Closed Source'
    elif 'ESM' in model_name:
        return 'AI for Science,Open-Source AI'
    elif 'EXAONE' in model_name:
        return 'Reasoning-Mathematics,Open-Source AI'
    elif 'Fugatto' in model_name:
        return 'AI Generation,Open-Source AI'
    elif 'Gemin' in model_name:
        return 'Multimodal AI,Closed Source'
    elif 'GLM' in model_name:
        return 'Large-Scale,Closed Source'
    elif 'GPT-4.5' in model_name:
        return 'Large-Scale,Multimodal AI,Closed Source'
    elif 'GPT-4o' in model_name:
        return 'Multimodal AI,Closed Source,Efficiency'
    elif 'GR' in model_name:
        return 'Robotics AI,Open-Source AI'
    elif 'Hairuo' in model_name:
        return 'Domain-Specific'
    elif 'Hunyuan' in model_name:
        return 'Large-Scale,Open-Source AI,Multimodal AI'
    elif 'Infinity' in model_name:
        return 'AI Generation,Open-Source AI'
    elif ('INTELLECT' in model_name) or ('k0' in model_name):
        return 'Reasoning-Mathematics,Closed Source'
    elif 'LLaVA-OV-72B' in model_name:
        return 'Multimodal AI,Open-Source AI'
    elif 'Llama 3' in model_name:
        return 'Large-Scale,Open-Source AI,Multimodal AI'
    elif 'Llama 4' in model_name:
        return 'Ultra-Large-Scale,Multimodal AI,Closed Source'
    elif 'Mathstral' in model_name:
        return 'Reasoning-Mathematics,Open-Source AI'
    elif 'Mercury' in model_name:
        return 'Efficiency,Closed Source'
    elif 'Mistral' in model_name:
        return 'Large-Scale,Domain-Specific'
    elif ('Movie' in model_name) or ('Gen' in model_name):
        return 'AI Generation'
    elif 'Nemotron' in model_name:
        return 'Large-Scale,Open-Source AI'
    elif 'NVLM' in model_name:
        return 'Multimodal AI,Open-Source AI'
    elif 'Octo' in model_name:
        return 'Robotics AI,Open-Source AI'
    elif ('o1' in model_name) or ('o3' in model_name):
        return 'Reasoning-Mathematics,Efficiency'
    elif 'Open' in model_name:
        return 'Robotics AI,Open-Source AI'
    elif 'Pangu' in model_name:
        return 'Large-Scale'
    elif 'Palmyra' in model_name:
        return 'Domain-Specific'
    elif 'Pixtral' in model_name:
        return 'Multimodal AI,Open-Source AI'
    elif 'Qwen' in model_name:
        return 'Large-Scale,Open-Source AI'
    elif 'QwQ' in model_name:
        return 'Reasoning-Mathematics,Open-Source AI'
    elif 'Table' in model_name:
        return 'Robotics AI'
    elif ('Veo' in model_name) or ('Wan' in model_name):
        return 'AI Generation'
    elif 'Yi' in model_name:
        return 'Large-Scale,Efficiency'
    else:
        return 'Other'

if 'Model' in df.columns:
    df['AI_Trend'] = df['Model'].apply(classify_trend)
    df['AI_Trend'] = df['AI_Trend'].str.split(',')
    df = df.explode('AI_Trend')
    df['AI_Trend'] = df['AI_Trend'].str.strip()
    st.write(f"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù„Ù€ AI_Trend Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£ÙˆÙ„ÙŠ: {df['AI_Trend'].unique()}")
else:
    st.warning("Ø¹Ù…ÙˆØ¯ 'Model' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ù„Ù† ÙŠØªÙ… ØªØµÙ†ÙŠÙ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.")
    df['AI_Trend'] = 'Other'

if 'Publication date' in df.columns:
    df['Publication date'] = pd.to_datetime(df['Publication date'], errors='coerce')
    df['Year'] = df['Publication date'].dt.year
else:
    st.warning("Ø¹Ù…ÙˆØ¯ 'Publication date' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ù„Ù† ÙŠØªÙ… Ø­Ø³Ø§Ø¨ Ø§Ù„Ø³Ù†Ø©.")
    df['Year'] = 0

# **Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø­Ø°Ù `df.to_csv` Ù‡Ù†Ø§ Ù„Ù…Ù†Ø¹ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„ÙˆÙŠØ¨.**

trend_keywords = {
    'AI Generation': ['generate', 'generative', 'GAN', 'diffusion', 'image-to-image', 'synthesis'],
    'Efficiency': ['efficient', 'efficiency', 'dropout', 'compress', 'optimize', 'low-resource', 'reduce'],
    'Multimodal AI': ['multimodal', 'vision-language', 'text and image', 'audio and text'],
    'Robotics AI': ['robot', 'embodied', 'navigation', 'control', 'manipulation'],
    'Reasoning-Mathematics': ['math', 'reasoning', 'proof', 'solve equation'],
    'AI for Science': ['biology', 'chemistry', 'protein', 'scientific', 'molecule'],
    'Closed Source': ['not released', 'private', 'not available'],
    'Open-Source AI': ['open-source', 'released', 'public', 'github'],
    'Large-Scale': ['large dataset', 'scale', 'GPT', 'BERT', 'Transformer'],
    'Ultra-Large-Scale': ['100B', 'trillion', 'huge model', 'massive'],
    'On-Device AI': ['on-device', 'edge', 'mobile', 'low-power'],
    'AI Regulation': ['policy', 'ethics', 'governance', 'regulation', 'safety'],
    'Domain-Specific': ['medical', 'legal', 'finance', 'healthcare', 'education'],
}

def classify_ai_trend_reclassify(text):
    if pd.isna(text):
        return "Other"
    text = str(text).lower()
    for trend, keywords in trend_keywords.items():
        if any(keyword in text for keyword in keywords):
            return trend
    return "Other"

if 'AI_Trend' in df.columns and 'Abstract' in df.columns and 'Model' in df.columns:
    mask_other = df['AI_Trend'] == "Other"
    df.loc[mask_other, 'AI_Trend_Reclassified'] = (
        df.loc[mask_other, ['Abstract', 'Model']]
        .fillna('')
        .agg(' '.join, axis=1)
        .apply(classify_ai_trend_reclassify)
    )

    df['AI_Trend'] = df.apply(
        lambda row: row['AI_Trend_Reclassified'] if row['AI_Trend'] == "Other" and row['AI_Trend_Reclassified'] != "Other"
        else row['AI_Trend'],
        axis=1
    )
    df.drop(columns=['AI_Trend_Reclassified'], inplace=True, errors='ignore')
else:
    st.warning("Ø£Ø¹Ù…Ø¯Ø© Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØµÙ†ÙŠÙ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (AI_Trend, Abstract, Model) Ù…ÙÙ‚ÙˆØ¯Ø©.")

# **Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø­Ø°Ù `df.to_csv` Ù‡Ù†Ø§ Ù„Ù…Ù†Ø¹ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„ÙˆÙŠØ¨.**


# ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø³Ù†ÙˆÙŠØ© (Ø¨ÙˆØ§Ø³Ø·Ø© Sedrah)
df = df[df['AI_Trend'] != 'Other']
if not df.empty and 'Year' in df.columns and 'AI_Trend' in df.columns:
    trend_by_year = df.groupby(['Year', 'AI_Trend']).size().reset_index(name='Model_Count')
    st.subheader("Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø­Ø³Ø¨ Ø§Ù„Ø³Ù†Ø©")
    st.dataframe(trend_by_year.head())
else:
    st.warning("DataFrame ÙØ§Ø±Øº Ø£Ùˆ Ø£Ø¹Ù…Ø¯Ø© 'Year'/'AI_Trend' Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ©. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø³Ù†ÙˆÙŠØ©.")
    trend_by_year = pd.DataFrame(columns=['Year', 'AI_Trend', 'Model_Count'])


# ØªØµÙˆØ± Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª (Ø¨ÙˆØ§Ø³Ø·Ø© Noora)
if 'AI_Trend' in df.columns and not df.empty and not trend_by_year.empty:
    AI_Trends_Frequency_df= df['AI_Trend'].value_counts().reset_index()
    AI_Trends_Frequency_df.columns = ['AI_Trend', 'Frequency']
    st.subheader("ØªÙƒØ±Ø§Ø± ÙƒÙ„ Ø§ØªØ¬Ø§Ù‡ Ù…Ù† Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
    st.dataframe(AI_Trends_Frequency_df)

    top_5_trends = AI_Trends_Frequency_df.nlargest(5, columns='Frequency')['AI_Trend'].tolist()
    filtered_top_5 = trend_by_year[trend_by_year['AI_Trend'].isin(top_5_trends)]

    if not filtered_top_5.empty:
        sns.set(style="whitegrid")
        fig_top5, ax_top5 = plt.subplots(figsize=(14, 7))
        sns.lineplot(data=filtered_top_5, x='Year', y='Model_Count', hue='AI_Trend', marker='o', ax=ax_top5)
        ax_top5.set_title('ØªØ·ÙˆØ± Ø£Ù‡Ù… 5 Ø§ØªØ¬Ø§Ù‡Ø§Øª Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª')
        ax_top5.set_xlabel('Ø§Ù„Ø³Ù†Ø©')
        ax_top5.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬')
        ax_top5.tick_params(axis='x', rotation=45)
        ax_top5.legend(title='Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ')
        plt.tight_layout()
        st.pyplot(fig_top5)
        plt.close(fig_top5)
    else:
        st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø±Ø³Ù… Ø£Ù‡Ù… 5 Ø§ØªØ¬Ø§Ù‡Ø§Øª Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ©.")
else:
    st.warning("Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ø³Ø§Ø¨ ØªÙƒØ±Ø§Ø± Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ø§Ù„Ø¹Ù…ÙˆØ¯ 'AI_Trend' Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ DataFrame ÙØ§Ø±Øº.")


# ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„ (Ø¨ÙˆØ§Ø³Ø·Ø© Noora)
st.subheader("ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„")
if 'Domain' in df.columns and not df.empty:
    df_clean = df.dropna(subset=['Domain'])
    df_exploded = df_clean.assign(Domain=df_clean['Domain'].str.split(',')).explode('Domain')
    df_exploded['Domain'] = df_exploded['Domain'].str.strip()

    if 'Year' in df_exploded.columns:
        domain_trends = df_exploded.groupby(['Year', 'Domain']).size().unstack(fill_value=0)
        st.write("Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„ (Ø§Ù„Ø°ÙŠÙ„):")
        st.dataframe(domain_trends.tail())
    else:
        st.warning("Ø¹Ù…ÙˆØ¯ 'Year' Ù…ÙÙ‚ÙˆØ¯ ÙÙŠ DataFrame Ø§Ù„Ù…Ø¬Ø§Ù„. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø³Ù†Ø©.")
        domain_trends = pd.DataFrame()
else:
    st.warning("Ø¹Ù…ÙˆØ¯ 'Domain' Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ DataFrame ÙØ§Ø±Øº. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„.")
    domain_trends = pd.DataFrame()


# ØªØµÙˆØ± Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„ (Ø¨ÙˆØ§Ø³Ø·Ø© Noora)
st.subheader("Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ© Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„")
if not domain_trends.empty:
    sns.set(style="whitegrid")
    fig_top_domains, ax_top_domains = plt.subplots(figsize=(14, 6))
    top_domains = domain_trends.sum().sort_values(ascending=False).head(5).index
    if not top_domains.empty:
        domain_trends[top_domains].plot(marker='o', linewidth=1, ax=ax_top_domains)
        ax_top_domains.set_title("Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø£Ù‡Ù… 5 Ù…Ø¬Ø§Ù„Ø§Øª Ø´Ø¹Ø¨ÙŠØ© (2021â€“2025)")
        ax_top_domains.set_xlabel("Ø§Ù„Ø³Ù†Ø©")
        ax_top_domains.set_ylabel("Ø¹Ø¯Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
        ax_top_domains.legend(title="Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
        plt.tight_layout()
        st.pyplot(fig_top_domains)
        plt.close(fig_top_domains)
    else:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ø§Ù„Ø§Øª Ø¹Ù„ÙŠØ§ Ù„Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ.")

    fig_all_domains, ax_all_domains = plt.subplots(figsize=(16, 8))
    domain_trends.plot(marker='.', linewidth=1, alpha=0.7, ax=ax_all_domains, legend=False)
    ax_all_domains.set_title("Ø§ØªØ¬Ø§Ù‡Ø§Øª Ù…Ø¬Ø§Ù„Ø§Øª Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (2021â€“2025)")
    ax_all_domains.set_xlabel("Ø§Ù„Ø³Ù†Ø©")
    ax_all_domains.set_ylabel("Ø¹Ø¯Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ")
    plt.tight_layout()
    st.pyplot(fig_all_domains)
    plt.close(fig_all_domains)
else:
    st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø¬Ø§Ù„ Ù„ØªØµÙˆØ±Ù‡Ø§.")


# ØªØ·ÙˆØ± Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ÙØ±Ø¯ÙŠØ© (Ø¨ÙˆØ§Ø³Ø·Ø© Sedrah)
st.subheader("ØªØ·ÙˆØ± Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ÙØ±Ø¯ÙŠØ©")
if 'AI_Trend' in df.columns and not df.empty and 'trend_by_year' in locals() and not trend_by_year.empty:
    all_trends = df['AI_Trend'].unique()
    filtered = trend_by_year[trend_by_year['AI_Trend'].isin(all_trends)]

    for trend in all_trends:
        fig_single_trend, ax_single_trend = plt.subplots(figsize=(10, 5))
        data = filtered[filtered['AI_Trend'] == trend]
        if not data.empty:
            sns.lineplot(data=data, x='Year', y='Model_Count', marker='o', ax=ax_single_trend)
            ax_single_trend.set_title(f'ØªØ·ÙˆØ± {trend} Ø¹Ø¨Ø± Ø§Ù„Ø³Ù†ÙˆØ§Øª')
            ax_single_trend.set_ylabel(trend)
            ax_single_trend.tick_params(axis='x', rotation=45)
            plt.tight_layout()
            st.pyplot(fig_single_trend)
            plt.close(fig_single_trend)
        else:
            st.write(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ø§ØªØ¬Ø§Ù‡: {trend}")
else:
    st.warning("Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¹Ø±Ø¶ ØªØ·ÙˆØ± Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©. Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ø£Ùˆ Ø£Ø¹Ù…Ø¯Ø© Ù…ÙÙ‚ÙˆØ¯Ø©.")


# Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø¹Ø§Ù… 2026 (Ø¨ÙˆØ§Ø³Ø·Ø© Noora)
st.subheader("Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø¹Ø§Ù… 2026")

if 'top_5_trends' in locals() and top_5_trends and 'trend_by_year' in locals() and not trend_by_year.empty:
    st.write(f"Ø£Ù‡Ù… 5 Ø§ØªØ¬Ø§Ù‡Ø§Øª Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡Ø§ Ù„Ù„ØªÙ†Ø¨Ø¤: {top_5_trends}")

    filtered_trends_2021_2025 = trend_by_year[
        (trend_by_year['AI_Trend'].isin(top_5_trends)) &
        (trend_by_year['Year'] >= 2021) &
        (trend_by_year['Year'] <= 2025)
    ].copy()

    st.write("Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…ØµÙØ§Ø© (2021-2025) Ù„Ø£Ù‡Ù… 5 Ø§ØªØ¬Ø§Ù‡Ø§Øª:")
    st.dataframe(filtered_trends_2021_2025)

    trend_models = {}
    trend_predictions_2026 = {}

    for trend in top_5_trends:
        trend_data = filtered_trends_2021_2025[filtered_trends_2021_2025['AI_Trend'] == trend]

        if not trend_data.empty and len(trend_data) >= 2:
            X = trend_data['Year'].values.reshape(-1, 1)
            y = trend_data['Model_Count'].values
            # st.write(f"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {trend} (X): {X}")
            # st.write(f"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {trend} (y): {y}")

            model = LinearRegression()
            model.fit(X, y)
            trend_models[trend] = model
            prediction_2026 = model.predict([[2026]])
            trend_predictions_2026[trend] = max(0, int(round(prediction_2026[0])))
        else:
            st.write(f"Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„Ø§ØªØ¬Ø§Ù‡: {trend} ÙÙŠ Ø§Ù„Ø£Ø¹ÙˆØ§Ù… 2021-2025 Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ù‚Ø§Ø·.")
            trend_predictions_2026[trend] = 0

    st.write("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù„Ø£Ù‡Ù… 5 Ø§ØªØ¬Ø§Ù‡Ø§Øª.")
    st.write("Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù„Ø¹Ø§Ù… 2026:")
    for trend, count in trend_predictions_2026.items():
        st.write(f"{trend}: {count}")

    predictions_df = pd.DataFrame(list(trend_predictions_2026.items()), columns=['AI_Trend', 'Predicted_Model_Count_2026'])
    predictions_df = predictions_df.sort_values(by='Predicted_Model_Count_2026', ascending=False)

    sns.set(style="whitegrid")
    fig_pred_bar, ax_pred_bar = plt.subplots(figsize=(10, 6))
    sns.barplot(data=predictions_df, x='Predicted_Model_Count_2026', y='AI_Trend', palette='viridis', ax=ax_pred_bar)
    ax_pred_bar.set_title('Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ø£Ù‡Ù… 5 Ø§ØªØ¬Ø§Ù‡Ø§Øª Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø¹Ø§Ù… 2026')
    ax_pred_bar.set_xlabel('Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹')
    ax_pred_bar.set_ylabel('Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ')
    plt.tight_layout()
    st.pyplot(fig_pred_bar)
    plt.close(fig_pred_bar)
else:
    st.warning("Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±. Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø£Ù‡Ù… 5 Ø§ØªØ¬Ø§Ù‡Ø§Øª Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø£Ùˆ Ø£Ù†Ù‡Ø§ ÙØ§Ø±ØºØ©ØŒ Ø£Ùˆ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø³Ù†ÙˆÙŠØ© ØºÙŠØ± Ù…ØªØ§Ø­Ø©.")


# Ø£Ù‡Ù… Ù…Ù†Ø¸Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ø¨ÙˆØ§Ø³Ø·Ø© Basmaleh)
st.subheader("Ø£Ù‡Ù… Ù…Ù†Ø¸Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
if 'Organization' in df.columns and not df.empty:
    org_counts = df['Organization'].value_counts().reset_index()
    org_counts.columns = ['Organization', 'Model_Count']

    st.write("Ø£ÙØ¶Ù„ Ù…Ù†ØªØ¬ÙŠ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:")
    st.dataframe(org_counts.head(10))

    fig_org_top10, ax_org_top10 = plt.subplots(figsize=(12, 6))
    sns.barplot(data=org_counts.head(10), x='Model_Count', y='Organization', palette='viridis', ax=ax_org_top10)
    ax_org_top10.set_title('Ø£Ù‡Ù… 10 Ù…Ù†Ø¸Ù…Ø§Øª Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬')
    ax_org_top10.set_xlabel('Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬')
    ax_org_top10.set_ylabel('Ø§Ù„Ù…Ù†Ø¸Ù…Ø©')
    plt.tight_layout()
    st.pyplot(fig_org_top10)
    plt.close(fig_org_top10)
else:
    st.warning("Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¹Ø±Ø¶ Ø£Ù‡Ù… Ø§Ù„Ù…Ù†Ø¸Ù…Ø§Øª. Ø¹Ù…ÙˆØ¯ 'Organization' Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ DataFrame ÙØ§Ø±Øº.")
'''

# Ø§Ø³ØªØ®Ø¯Ù… code_editor Ù„ØªØ¬Ø±Ø¨Ø© ØªØ­Ø±ÙŠØ± Ø£ÙØ¶Ù„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
try:
    from code_editor import code_editor
    response_dict = code_editor(cody, lang="python", height=800,
                                editor_props={"theme": "dracula"})
    user_code_from_editor = response_dict['text'] if response_dict and 'text' in response_dict else cody
except ImportError:
    st.warning("`streamlit-code-editor` ØºÙŠØ± Ù…Ø«Ø¨Øª. Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ«Ø¨ÙŠØªÙ‡ (pip install streamlit-code-editor) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ¬Ø±Ø¨Ø© Ù…Ø­Ø±Ø± Ø£ÙØ¶Ù„ØŒ Ø£Ùˆ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… `st.text_area`.")
    user_code_from_editor = st.text_area("âœï¸ Ø¹Ø¯Ù‘Ù„ Ø§Ù„ÙƒÙˆØ¯ Ù‡Ù†Ø§:", cody, height=800)


# --- Ø²Ø± Ø§Ù„ØªØ´ØºÙŠÙ„ ---
if st.button("ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„"):
    if not user_code_from_editor.strip():
        final_user_code = cody
    else:
        final_user_code = user_code_from_editor

    if 'df' in locals() and df is not None: # ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ df
        try:
            st.empty() # Ù…Ø³Ø­ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©

            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ù† DataFrame Ù„ØªØ¬Ù†Ø¨ ØªØ¹Ø¯ÙŠÙ„ df Ø§Ù„Ø£ØµÙ„ÙŠ Ø¨ÙˆØ§Ø³Ø·Ø© ÙƒÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            df_for_exec = df.copy()

            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ÙÙŠÙ‡Ø§ ØªÙ†ÙÙŠØ° ÙƒÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            exec_globals = {
                'st': st, 'pd': pd, 'np': np, 'plt': plt, 'sns': sns,
                'df': df_for_exec, # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©
                'StandardScaler': StandardScaler, 'OneHotEncoder': OneHotEncoder, 'LabelEncoder': LabelEncoder,
                'ColumnTransformer': ColumnTransformer, 'Pipeline': Pipeline,
                'SimpleImputer': SimpleImputer, 'KMeans': KMeans, 'PCA': PCA,
                'train_test_split': train_test_split, 'RandomForestClassifier': RandomForestClassifier,
                'accuracy_score': accuracy_score, 'precision_score': precision_score,
                'recall_score': recall_score, 'f1_score': f1_score, 'confusion_matrix': confusion_matrix,
                'classification_report': classification_report,
                'LinearRegression': LinearRegression, 'KNeighborsClassifier': KNeighborsClassifier
            }
            exec(final_user_code, exec_globals) # ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯

            st.success("ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯ Ø¨Ù†Ø¬Ø§Ø­!")
            st.write("---")
            st.write("Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø£Ø¹Ù„Ø§Ù‡ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„.")

        except Exception as e:
            st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙˆØ¯: {e}")
            st.exception(e) # Ù„Ø¹Ø±Ø¶ ØªØªØ¨Ø¹ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ÙƒØ§Ù…Ù„
    else:
        st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹.")
